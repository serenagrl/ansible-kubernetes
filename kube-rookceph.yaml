# Author: Serena Yeoh
#
# Disclaimer:
# This playbook was written based on my self-learning and may not follow certain
# best practices or work properly in your environment. Use it at your own risk.
#
- name: Create Virtual Hard Disk and Attach to VMs
  hosts: winrm
  gather_facts: no

  tasks:
    # You need to remove the disks manually if you wish to rerun this.
    - name: Create Hyper-V virtual disks and attach them to the nodes
      vars:
        hostnames: "{% if (groups['kubernetes_nodes'] | length > 0) %}kubernetes_nodes{% else %}kubernetes_control_planes{% endif %}"
        vhd_disk: "{{ _rook_ceph.disk.dir }}\\{{ item }}\\Virtual Hard Disks\\{{ _rook_ceph.disk.name }}.vhdx"
      ansible.windows.win_powershell:
        script: |
          New-VHD -Path "{{ vhd_disk }}" -SizeBytes {{ _rook_ceph.disk.size }} -Dynamic
          Add-VMHardDiskDrive {{ item }} -Path "{{ vhd_disk }}"
        creates: "{{ vhd_disk }}"
      loop: "{{ q('inventory_hostnames', hostnames) }}"

- name: <<<<< Install and configure Rook-Ceph >>>>>
  hosts: kubernetes_control_planes[0]
  become_user: "{{ _kube.user }}"
  
  vars:
    repo_dir: "{{ _work_dir }}/rook"
    deploy_dir: "{{ repo_dir }}/deploy/examples"
    config_file: "{{ _work_dir }}/rook-config-override.yaml"

  tasks:
    - name: Download and configure rook-ceph
      become_user: root
      block:
        - import_tasks: ./tasks/task-git-clone.yaml
          vars:
            name: "rook ceph"
            repo_path: "rook/rook"
            pkg_version: "{{ _pkg.rook_ceph | default }}"
            version_prefix: "v"
            url: "https://github.com/rook/rook.git"
            dest: "{{ repo_dir }}"

        - name: Scale down the pods for lesser than 3 kubernetes_nodes configuration
          vars:
            node_count: "{% if (groups['kubernetes_nodes'] | length) > 0 %}{{ groups['kubernetes_nodes'] | length }}{% else %}{{ groups['kubernetes_control_planes'] | length }}{% endif %}"
          shell: |
                  sed -i 's/count: 2$/count: 1/g' cluster.yaml
                  sed -i 's/count: 3$/count: 2/g' cluster.yaml
          args:
            chdir: "{{ deploy_dir }}"
          when: node_count | int < 3

        - name: Configure manifest values
          shell: |
                  sed -i 's/ROOK_LOG_LEVEL: "INFO"$/ROOK_LOG_LEVEL: "WARNING"/g' operator.yaml
          args:
            chdir: "{{ deploy_dir }}"

        - name: Create rook config override 
          blockinfile:
            create: yes
            path: "{{ config_file }}"
            block: |
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: rook-config-override
                namespace: rook-ceph
              data:
                config: |
                  [global]
                  clog_to_syslog = true
                  clog_to_syslog_level = "warn"
                  clog_to_syslog_facility = "daemon"
                  mon_cluster_log_file_level = "warn"
                  mon_cluster_log_to_stderr = false
                  mon_cluster_log_to_syslog = true
                  mon_cluster_log_to_syslog_level = "warn"
                  mon_cluster_log_to_syslog_facility = "daemon"
                  log_to_stderr = false
                  log_to_syslog = true
                  err_to_syslog = true

      delegate_to: localhost

    - name: Installing rook-ceph custom resources and operator
      kubernetes.core.k8s:
        force: yes
        definition: "{{ lookup('file', item) | from_yaml_all }}"
        wait: yes
        wait_timeout: 300
      loop:
        - "{{ deploy_dir }}/crds.yaml"
        - "{{ deploy_dir }}/common.yaml"
        - "{{ config_file }}"
        - "{{ deploy_dir }}/operator.yaml"

    - name: Waiting for rook-ceph operator to be ready
      shell: kubectl rollout status -n rook-ceph deployment/rook-ceph-operator

    - name: Installing rook-ceph cluster
      kubernetes.core.k8s:
        force: yes
        definition: "{{ lookup('file', item) | from_yaml_all }}"
        wait: yes
        wait_timeout: 300
      loop:
        - "{{ deploy_dir }}/cluster.yaml"

    - name: Waiting for ceph cluster to be ready (may take a while)
      shell: |
              kubectl wait --for=jsonpath={.status.phase}=Ready cephclusters rook-ceph -n rook-ceph --timeout=900s

    - name: Installing rook-ceph toolbox
      kubernetes.core.k8s:
        force: yes
        definition: "{{ lookup('file', '{{ item }}') | from_yaml_all }}"
        wait: yes
      loop:
        - "{{ deploy_dir }}/toolbox.yaml"

    - name: Get ceph toolbox pod information
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: rook-ceph
        label_selectors:
          - app = rook-ceph-tools
      register: pod_info

    - name: Create new storage pool
      vars:
        pod_name: "{{ pod_info | json_query('resources[0].metadata.name') }}"
        node_count: "{% if (groups['kubernetes_nodes'] | length) > 0 %}{{ groups['kubernetes_nodes'] | length}}{% else %}{{ groups['kubernetes_control_planes'] | length }}{% endif %}"

      kubernetes.core.k8s_exec:
        namespace: rook-ceph
        pod: '{{ pod_name }}'
        command: '{{ item }}'
      when: node_count | int > 1
      loop:
        - "ceph osd pool create {{ _rook_ceph.pool_name }} 128"
        - "ceph osd pool set {{ _rook_ceph.pool_name }} min_size {{ (node_count | int) - 1 }}"
        - "ceph osd pool set {{ _rook_ceph.pool_name }} size {{ node_count }}"

    - name: Configure file system and storage class
      become_user: root
      block:
        - name: Set filesystem name and pvc sizes
          shell: |
                  sed -i 's/name: myfs$/name: {{ _rook_ceph.fs_name }}/g' filesystem.yaml
                  sed -i 's/myfs/{{ _rook_ceph.fs_name }}/g' ./csi/cephfs/storageclass.yaml
                  sed -i 's/storage: 1Gi$/storage: {{ _rook_ceph.pvc.fs_size }}/g' ./csi/cephfs/pvc.yaml
                  sed -i 's/storage: 1Gi$/storage: {{ _rook_ceph.pvc.rbd_size }}/g' ./csi/rbd/pvc.yaml
          args:
            chdir: "{{ deploy_dir }}"

        - name: Scale down replicas for lesser than 3 kubernetes_nodes configuration
          shell: |
                  sed -i 's/size: 3$/size: 2/g' filesystem.yaml
          args:
            chdir: "{{ deploy_dir }}"
          when: (groups['kubernetes_nodes'] | length) < 3

      delegate_to: localhost

    - name: Create ceph fs and rbd storage classes
      kubernetes.core.k8s:
        apply: yes
        force: yes
        definition: "{{ lookup('file', '{{ item }}') | from_yaml_all }}"
        wait: yes
      loop:
        - "{{ deploy_dir }}/filesystem.yaml"
        - "{{ deploy_dir }}/csi/cephfs/storageclass.yaml"
        - "{{ deploy_dir }}/csi/rbd/storageclass.yaml"

    - name: Create ceph fs and rbd pvcs
      kubernetes.core.k8s:
        apply: yes
        force: yes
        namespace: "{{ _rook_ceph.pvc.namespace }}"
        definition: "{{ lookup('file', '{{ item }}') | from_yaml_all }}"
        wait: yes
      loop:
        - "{{ deploy_dir }}/csi/cephfs/pvc.yaml"
        - "{{ deploy_dir }}/csi/rbd/pvc.yaml"

    - name: Set to default storage class to {{ _rook_ceph.is_default }}
      kubernetes.core.k8s:
        state: patched
        kind: StorageClass
        name: "rook-cephfs"
        definition: |
          metadata:
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
      when: _rook_ceph.is_default

    - name: Configure nodeport
      block:
        - name: Expose ceph dashboard
          kubernetes.core.k8s:
            apply: yes
            force: yes
            definition: |
              apiVersion: v1
              kind: Service
              metadata:
                name: rook-ceph-mgr-dashboard-external-https
                namespace: rook-ceph
                labels:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
              spec:
                ports:
                - name: dashboard
                  nodePort: {{ _rook_ceph.nodeport }}
                  port: 8443
                  protocol: TCP
                  targetPort: 8443
                selector:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
                sessionAffinity: None
                type: NodePort
    
        - name: Reset ceph dashboard service
          kubernetes.core.k8s:
            state: absent
            kind: Service
            namespace: rook-ceph
            name: rook-ceph-mgr-dashboard
      
      when: _kube.use_nodeport

    - name: Configure Ceph dashboard
      vars:
        pod_name: "{{ pod_info | json_query('resources[0].metadata.name') }}"
      kubernetes.core.k8s_exec:
        namespace: rook-ceph
        pod: '{{ pod_name }}'
        command: '{{ item }}'
      loop:
        - "ceph mgr module enable rook"
        - "ceph orch set backend rook"
        - "ceph dashboard set-alertmanager-api-host 'http://alertmanager-main.monitoring.svc:9093/alerts'"
        - "ceph dashboard set-prometheus-api-host 'http://prometheus-k8s.monitoring.svc:9090/prometheus'"
        - "ceph dashboard set-prometheus-api-ssl-verify False"
        - "ceph dashboard set-alertmanager-api-ssl-verify False"

    - import_tasks: ./tasks/task-get-secret.yaml
      vars:
        name: "ceph dashboard password"
        command: "kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath='{.data.password}'"
        filename: "ceph_dashboard_password"

    - import_tasks: ./tasks/task-create-ingress-route.yaml
      vars:
        namespace: rook-ceph
        name: ceph-dashboard-ingress
        protocol: HTTPS
        route: ceph
        service_name: rook-ceph-mgr-dashboard
        port: 8443

    - name: Configure prometheus to scrape ceph metrics endpoint
      block:
        - name: Create additional-scrape-config secret to scrape ceph endpoint
          vars:
            scrape_config: |
              - job_name: 'ceph'
                static_configs:
                  - targets: ['rook-ceph-mgr.rook-ceph.svc:9283']

          kubernetes.core.k8s:
            state: present
            definition: |
              apiVersion: v1
              kind: Secret
              type: Opaque             
              metadata:
                name: "additional-scrape-configs"
                namespace: "monitoring"     
              data:
                prometheus-additional.yaml: "{{ scrape_config | from_yaml | b64encode }}"

        - name: Patch prometheus to include additional scrape configs          
          kubernetes.core.k8s:
            state: patched
            namespace: monitoring
            name: k8s
            kind: Prometheus
            definition: |
              spec:
                additionalScrapeConfigs:
                  name: additional-scrape-configs
                  key: prometheus-additional.yaml
      when: _kube.install.monitoring

    - name: Cleanup directories
      become_user: root
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - "{{ config_file }}"
        - "{{ repo_dir }}"
      delegate_to: localhost
      when: _rook_ceph.cleanup
