# Author: Serena Yeoh
#
# Disclaimer:
# This playbook was written based on my self-learning and may not follow certain
# best practices or work properly in your environment. Use it at your own risk.
#

- name: Check for existing Virtual Hard Disks
  ansible.builtin.include_tasks: ./tasks/task-check-vhdx-exist.yaml
  vars:
    win_folder: "{{ csi_ceph.disk.vm_folder }}"
    host_name: "{{ hostvars[item].ansible_hostname }}"
    disk_name: "{{ csi_ceph.disk.name }}"
    hostnames: "{% if (groups['kubernetes_control_planes'] | length >= 3) %}kubernetes_control_planes{% else %}kubernetes_cluster{% endif %}"
  loop: "{{ q('inventory_hostnames', hostnames) }}"

# IMPORTANT! You need to remove the disks manually if you wish to rerun this.
- name: Create Hyper-V virtual disks and attach them to nodes
  delegate_to: "{{ hostvars[item].winrm_host }}"
  vars:
    hostname: "{{ hostvars[item].ansible_hostname }}"
    hostnames: "{% if (groups['kubernetes_control_planes'] | length >= 3) %}kubernetes_control_planes{% else %}kubernetes_cluster{% endif %}"
    vhd_disk: "{{ csi_ceph.disk.vm_folder }}\\{{ hostname }}\\Virtual Hard Disks\\{{ csi_ceph.disk.name }}.vhdx"
  ansible.windows.win_powershell:
    script: |
      New-VHD -Path "{{ vhd_disk }}" -SizeBytes {{ csi_ceph.disk.size }} -Dynamic
      Add-VMHardDiskDrive {{ hostname }} -Path "{{ vhd_disk }}"
    creates: "{{ vhd_disk }}"
  loop: "{{ q('inventory_hostnames', hostnames) }}"
  run_once: yes

- name: Download and configure rook-ceph
  delegate_to: localhost
  become_user: root
  block:
    - ansible.builtin.import_tasks: ./tasks/task-git-clone.yaml
      vars:
        name: "rook ceph"
        repo_path: "rook/rook"
        pkg_version: "{{ csi_ceph.version | default }}"
        version_prefix: "v"
        url: "https://github.com/rook/rook.git"
        dest: "{{ REPO_DIR }}"

    - name: Scale down the pods for lesser than 3 kubernetes_worker_nodes configuration
      vars:
        node_count: "{% if (groups['kubernetes_worker_nodes'] | length) > 0 %}{{ groups['kubernetes_worker_nodes'] | length }}{% else %}{{ groups['kubernetes_control_planes'] | length }}{% endif %}"
      ansible.builtin.shell: |
              sed -i 's/count: 2$/count: 1/g' cluster.yaml
              sed -i 's/count: 3$/count: 2/g' cluster.yaml
      args:
        chdir: "{{ DEPLOY_DIR }}"
      when: node_count | int < 3

    - name: Configure manifest values
      ansible.builtin.shell: |
              sed -i 's/ROOK_LOG_LEVEL: "INFO"$/ROOK_LOG_LEVEL: "WARNING"/g' operator.yaml
      args:
        chdir: "{{ DEPLOY_DIR }}"

    - name: Patch tolerations and node selectors
      block:
        - name: Configure cluster to run on control-planes only 
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/cluster.yaml"
            regexp: '^(spec:)$'
            replace: |8-
                    \1
                      placement:
                        all:
                          nodeAffinity:
                           requiredDuringSchedulingIgnoredDuringExecution:
                             nodeSelectorTerms:
                             - matchExpressions:
                               - key: node-role.kubernetes.io/control-plane
                                 operator: Exists
                          tolerations:
                            - effect: NoSchedule
                              key: node-role.kubernetes.io/control-plane
                              operator: Exists

        - name: Configure operator to run on control-planes only
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/operator.yaml"
            regexp: '^(data:)$'
            replace: |8-
                    \1
                    {{ OPERATOR_TOLERATIONS_TEMPLATE }}

        - name: Configure operator tolerations and node affinity environment variables
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/operator.yaml"
            regexp: '^\s*(env:)$'
            replace: |2-
                        \1
                          - name: DISCOVER_TOLERATIONS
                            value: |
                              - effect: NoSchedule
                                key: node-role.kubernetes.io/control-plane
                                operator: Exists                    
                          - name: DISCOVER_AGENT_NODE_AFFINITY
                            value: "node-role.kubernetes.io/control-plane="

        - name: Configure operator deployment to run on control-planes only
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/operator.yaml"
            regexp: '^\s*(containers:)$'
            replace: |4-
                      tolerations:
                        - effect: NoSchedule
                          key: node-role.kubernetes.io/control-plane
                          operator: Exists
                      nodeSelector:
                        node-role.kubernetes.io/control-plane: ""
                      \1

        - name: Configure toolbox to run on control-planes only
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/toolbox.yaml"
            regexp: '^\s*(tolerations:)$'
            replace: |4-
                      nodeSelector:
                        node-role.kubernetes.io/control-plane: ""
                      \1
                        - effect: NoSchedule
                          key: node-role.kubernetes.io/control-plane
                          operator: Exists

        - name: Patch filesystem tolerations
          ansible.builtin.replace:
            path: "{{ DEPLOY_DIR }}/filesystem.yaml"
            regexp: '^\s*(placement:)$'
            replace: |6-
                      \1
                        nodeAffinity:
                          requiredDuringSchedulingIgnoredDuringExecution:
                            nodeSelectorTerms:
                            - matchExpressions:
                              - key: node-role.kubernetes.io/control-plane
                                operator: Exists

                        tolerations:
                          - effect: NoSchedule
                            key: node-role.kubernetes.io/control-plane
                            operator: Exists

      when: not _kube.allow_scheduling

    - name: Create rook config override 
      ansible.builtin.blockinfile:
        create: yes
        path: "{{ CONFIG_FILE }}"
        block: |
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: rook-config-override
            namespace: rook-ceph
          data:
            config: |
                      [global]
                      clog_to_syslog = false
                      clog_to_syslog_level = "warn"
                      clog_to_syslog_facility = "daemon"
                      mon_cluster_log_file = "/dev/null"
                      mon_cluster_log_file_level = "warn"
                      mon_cluster_log_to_stderr = false
                      mon_cluster_log_to_syslog = false
                      mon_cluster_log_to_syslog_level = "warn"
                      mon_cluster_log_to_syslog_facility = "daemon"
                      mon_health_to_clog = false
                      mon_health_detail_to_clog = false
                      restapi_log_level = "warn"
                      log_to_stderr = false
                      log_to_syslog = false
                      err_to_syslog = false
                      debug_alienstore = "0/0"
                      debug_asok = "0/0"
                      debug_auth = "0/0"
                      debug_bdev = "0/0"
                      debug_bluefs = "0/0"
                      debug_bluestore = "0/0"
                      debug_buffer = "0/0"
                      debug_cephfs_mirror = "0/0"
                      debug_cephsqlite = "0/0"
                      debug_civetweb = "0/0"
                      debug_client = "0/0"
                      debug_compressor = "0/0"
                      debug_context = "0/0"
                      debug_crush = "0/0"
                      debug_crypto = "0/0"
                      debug_dpdk = "0/0"
                      debug_eventtrace = "0/0"
                      debug_filer = "0/0"
                      debug_filestore = "0/0"
                      debug_finisher = "0/0"
                      debug_fuse = "0/0"
                      debug_heartbeatmap = "0/0"
                      debug_immutable_obj_cache = "0/0"
                      debug_javaclient = "0/0"
                      debug_journal = "0/0"
                      debug_journaler = "0/0"
                      debug_kstore = "0/0"
                      debug_leveldb = "0/0"
                      debug_lockdep = "0/0"
                      debug_mclock = "0/0"
                      debug_mds = "0/0"
                      debug_mds_balancer = "0/0"
                      debug_mds_locker = "0/0"
                      debug_mds_log = "0/0"
                      debug_mds_log_expire = "0/0"
                      debug_mds_migrator = "0/0"
                      debug_memdb = "0/0"
                      debug_mgr = "0/0"
                      debug_mgrc = "0/0"
                      debug_mon = "0/0"
                      debug_monc = "0/0"
                      debug_ms = "0/0"
                      debug_none = "0/0"
                      debug_objclass = "0/0"
                      debug_objectcacher = "0/0"
                      debug_objecter = "0/0"
                      debug_optracker = "0/0"
                      debug_osd = "0/0"
                      debug_paxos = "0/0"
                      debug_perfcounter = "0/0"
                      debug_prioritycache = "0/0"
                      debug_rados = "0/0"
                      debug_rbd = "0/0"
                      debug_rbd_mirror = "0/0"
                      debug_rbd_pwl = "0/0"
                      debug_rbd_replay = "0/0"
                      debug_refs = "0/0"
                      debug_reserver = "0/0"
                      debug_rgw = "0/0"
                      debug_rgw_datacache = "0/0"
                      debug_rgw_sync = "0/0"
                      debug_rocksdb = "0/0"
                      debug_seastore = "0/0"
                      debug_seastore_cache = "0/0"
                      debug_seastore_cleaner = "0/0"
                      debug_seastore_device = "0/0"
                      debug_seastore_journal = "0/0"
                      debug_seastore_lba = "0/0"
                      debug_seastore_odata = "0/0"
                      debug_seastore_omap = "0/0"
                      debug_seastore_onode = "0/0"
                      debug_seastore_tm = "0/0"
                      debug_striper = "0/0"
                      debug_test = "0/0"
                      debug_throttle = "0/0"
                      debug_timer = "0/0"
                      debug_tp = "0/0"

# Note: Need to remove crds on the kubernetes cluster if you need to rerun this.
- name: Installing rook-ceph custom resources and operator
  kubernetes.core.k8s:
    force: yes
    definition: "{{ lookup('file', item) | from_yaml_all }}"
    wait: yes
    wait_timeout: 300
  loop:
    - "{{ DEPLOY_DIR }}/crds.yaml"
    - "{{ DEPLOY_DIR }}/common.yaml"
    - "{{ CONFIG_FILE }}"
    - "{{ DEPLOY_DIR }}/operator.yaml"

- name: Waiting for rook-ceph operator to be ready
  ansible.builtin.shell: kubectl rollout status -n rook-ceph deployment/rook-ceph-operator

- name: Installing rook-ceph cluster
  kubernetes.core.k8s:
    force: yes
    definition: "{{ lookup('file', item) | from_yaml_all }}"
    wait: yes
    wait_timeout: 300
  loop:
    - "{{ DEPLOY_DIR }}/cluster.yaml"

- name: Waiting for ceph cluster to be ready (may timeout on slow servers)
  ansible.builtin.shell: kubectl wait --for=jsonpath={.status.phase}=Ready cephclusters rook-ceph -n rook-ceph --timeout=900s

- name: Installing rook-ceph toolbox
  kubernetes.core.k8s:
    force: yes
    definition: "{{ lookup('file', '{{ item }}') | from_yaml_all }}"
    wait: yes
  loop:
    - "{{ DEPLOY_DIR }}/toolbox.yaml"

- name: Get ceph toolbox pod information
  kubernetes.core.k8s_info:
    kind: Pod
    namespace: rook-ceph
    label_selectors:
      - app = rook-ceph-tools
  register: pod_info

- name: Create new storage pool
  vars:
    pod_name: "{{ pod_info | json_query('resources[0].metadata.name') }}"
    node_count: "{% if (groups['kubernetes_worker_nodes'] | length) > 0 %}{{ groups['kubernetes_worker_nodes'] | length}}{% else %}{{ groups['kubernetes_control_planes'] | length }}{% endif %}"

  kubernetes.core.k8s_exec:
    namespace: rook-ceph
    pod: '{{ pod_name }}'
    command: '{{ item }}'
  when: node_count | int > 1
  loop:
    - "ceph osd pool create {{ csi_ceph.pool_name }} 128"
    - "ceph osd pool set {{ csi_ceph.pool_name }} min_size {{ (node_count | int) - 1 }}"
    - "ceph osd pool set {{ csi_ceph.pool_name }} size {{ node_count }}"

- name: Configure file system and storage class
  delegate_to: localhost
  become_user: root
  block:
    - name: Set filesystem name and pvc sizes
      ansible.builtin.shell: |
              sed -i 's/name: myfs$/name: {{ csi_ceph.fs_name }}/g' filesystem.yaml
              sed -i 's/myfs/{{ csi_ceph.fs_name }}/g' ./csi/cephfs/storageclass.yaml
              sed -i 's/storage: 1Gi$/storage: {{ csi_ceph.pvc.fs_size }}/g' ./csi/cephfs/pvc.yaml
              sed -i 's/storage: 1Gi$/storage: {{ csi_ceph.pvc.rbd_size }}/g' ./csi/rbd/pvc.yaml
      args:
        chdir: "{{ DEPLOY_DIR }}"

    - name: Scale down replicas for lesser than 3 kubernetes_worker_nodes configuration
      ansible.builtin.shell: |
              sed -i 's/size: 3$/size: 2/g' filesystem.yaml
      args:
        chdir: "{{ DEPLOY_DIR }}"
      when: (groups['kubernetes_worker_nodes'] | length) < 3

- name: Create ceph fs and rbd storage classes
  kubernetes.core.k8s:
    apply: yes
    force: yes
    definition: "{{ lookup('file', item) | from_yaml_all }}"
    wait: yes
  loop:
    - "{{ DEPLOY_DIR }}/filesystem.yaml"
    - "{{ DEPLOY_DIR }}/csi/cephfs/storageclass.yaml"
    - "{{ DEPLOY_DIR }}/csi/rbd/storageclass.yaml"

- name: Create ceph fs and rbd pvcs
  kubernetes.core.k8s:
    apply: yes
    force: yes
    namespace: "{{ csi_ceph.pvc.namespace }}"
    definition: "{{ lookup('file', item) | from_yaml_all }}"
    wait: yes
  loop:
    - "{{ DEPLOY_DIR }}/csi/cephfs/pvc.yaml"
    - "{{ DEPLOY_DIR }}/csi/rbd/pvc.yaml"

- name: Set to default storage class to {{ csi_ceph.is_default }}
  kubernetes.core.k8s:
    state: patched
    kind: StorageClass
    name: "rook-cephfs"
    definition: |
      metadata:
        annotations:
          storageclass.kubernetes.io/is-default-class: "true"
  when: csi_ceph.is_default

- name: Configure Ceph dashboard
  vars:
    pod_name: "{{ pod_info | json_query('resources[0].metadata.name') }}"
  kubernetes.core.k8s_exec:
    namespace: rook-ceph
    pod: '{{ pod_name }}'
    command: '{{ item }}'
  loop:
    - "ceph mgr module enable rook"
    # - "ceph orch set backend rook"

- ansible.builtin.import_tasks: ./tasks/task-get-secret.yaml
  vars:
    name: "Ceph dashboard password"
    command: "kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath='{.data.password}'"

  # Notes: Use cluster's domain name and with custom route /ceph
- name: Setup ingress
  ansible.builtin.include_tasks: "{{ INGRESS_SETUP }}"
  vars:
    name: ceph-dashboard
    namespace: rook-ceph
    definition: "{{ INGRESS_TEMPLATE }}"  

- name: Check if Prometheus is installed
  ansible.builtin.shell: |
      kubectl get prometheus k8s -n monitoring -o json | jq .status.conditions[0].type -r
  register: prometheus_status
  ignore_errors: yes

- name: Configure prometheus to scrape ceph metrics endpoint
  block:
    - name: Configure prometheus endpoint
      vars:
        pod_name: "{{ pod_info | json_query('resources[0].metadata.name') }}"
      kubernetes.core.k8s_exec:
        namespace: rook-ceph
        pod: '{{ pod_name }}'
        command: '{{ item }}'
      loop:
        - "ceph dashboard set-alertmanager-api-host 'http://alertmanager-main.monitoring.svc:9093/alerts/'"
        - "ceph dashboard set-prometheus-api-host 'http://prometheus-k8s.monitoring.svc:9090/prometheus/'"
        - "ceph dashboard set-prometheus-api-ssl-verify False"
        - "ceph dashboard set-alertmanager-api-ssl-verify False"

    - name: Export existing additional scrape configs to file
      ansible.builtin.shell: |
        kubectl -n monitoring get secret additional-scrape-configs -o jsonpath="{['data']['prometheus-additional\.yaml']}" | base64 --decode > {{ SCRAPE_CONFIG }}
      args:
        chdir: $HOME  

    - name: Add istio scrape configs to scrape configs file
      ansible.builtin.blockinfile:
        path: "~/{{ SCRAPE_CONFIG }}"
        marker: "# {mark} CEPH METRICS BLOCK"
        block: |
          - job_name: 'ceph'
            static_configs:
              - targets: ['rook-ceph-mgr.rook-ceph.svc:9283']

    - name: Create secret for additional Prometheus scrape configs
      ansible.builtin.shell: |
        kubectl create secret generic additional-scrape-configs --from-file={{ SCRAPE_CONFIG }} --dry-run=client -o yaml > {{ SECRET_CONFIG }}
      args:
        chdir: $HOME  

    - name: Apply additional scrape configs
      kubernetes.core.k8s:
        namespace: monitoring
        src: "~/{{ SECRET_CONFIG }}"

    - name: Patch prometheus to include additional scrape configs          
      kubernetes.core.k8s:
        state: patched
        namespace: monitoring
        name: k8s
        kind: Prometheus
        definition: |
          spec:
            additionalScrapeConfigs:
              name: additional-scrape-configs
              key: prometheus-additional.yaml
  when: HAS_PROMETHEUS

- name: Cleanup
  block:
    - name: Cleanup config files
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - "~/{{ SCRAPE_CONFIG }}"
        - "~/{{ SECRET_CONFIG }}"

    - name: Cleanup directories
      become_user: root
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - "{{ CONFIG_FILE }}"
        - "{{ REPO_DIR }}"
      delegate_to: localhost
  when: _cleanup
