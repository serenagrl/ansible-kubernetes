# Author: Serena Yeoh
#
# Disclaimer:
# This playbook was written based on my self-learning and may not follow certain
# best practices or work properly in your environment. Use it at your own risk.
#
- name: <<<<< Create kubernetes cluster >>>>>
  hosts: kubernetes_control_planes[0]

  tasks:
    # Remove the --apiserver-advertise-address if you are pointing to a load balancer.
    - name: Initializing kubernetes cluster (primary control plane)
      vars:
        release: "{% if _kube.release is defined and _kube.release and _kube.release != 'latest' %}--kubernetes-version {{ _kube.release }}{% endif %}"
      shell: |
              kubeadm init --pod-network-cidr={{ _kube.cluster.cidr_network }}/16 \
              --upload-certs --cri-socket unix:///var/run/containerd/containerd.sock \
              --control-plane-endpoint={{ _kube.cluster.name }} {{ release }}  

    - name: Get join token for nodes
      shell: kubeadm token create --print-join-command
      register: join_node_command

    - name: Get certificate key for secondary control planes
      shell: |
        echo --control-plane --certificate-key $(sudo kubeadm init phase upload-certs --upload-certs | grep -vw -e certificate -e Namespace)
      register: join_control_plane_command

    - become_user: root
      block:
        - name: Copy join command for nodes to Ansible host
          copy:
            content: "{{ join_node_command.stdout_lines[0] }}"
            dest: "{{ _work_dir  }}/join_node_command"
            mode: 0777

        - name: Copy join command for control planes to Ansible host
          copy:
            content: "{{ join_node_command.stdout_lines[0] }} {{ join_control_plane_command.stdout_lines[0] }}"
            dest: "{{ _work_dir  }}/join_control_plane_command"
            mode: 0777

        - include_tasks: ./tasks/task-get-manifest.yaml
          vars:
            name: "calico"
            repo_path: "projectcalico/calico"
            pkg_version: "{{ _pkg.calico | default }}"
            version_prefix: "v"
            dest: "{{_work_dir}}" 
            manifest_url: "https://raw.githubusercontent.com/{{ repo_path }}/{{ release }}/manifests"
            urls:
              - "{{ manifest_url }}/tigera-operator.yaml"
              - "{{ manifest_url }}/custom-resources.yaml" 
          
        - name: Set CIDR network
          replace:
            path: "{{ _work_dir }}/custom-resources.yaml"
            regexp: '192.168.0.0'
            replace: '{{ _kube.cluster.cidr_network }}'
      
      delegate_to: localhost

- name: Join other control planes
  hosts: kubernetes_control_planes[1:99]
  vars:
    cmd_file: join_control_plane_command

  tasks:
    - block: 
        - name: Initializing other control planes (if available)
          copy:
            src: "{{ _work_dir }}/{{ cmd_file }}"
            dest: "/tmp/{{ cmd_file }}"
            mode: 0777

        - name: Joining other control planes to cluster
          command: sh '/tmp/{{ cmd_file }}'

      when: (groups['kubernetes_control_planes'] | length) > 1 

- name: Configuring kubernetes control planes
  hosts: kubernetes_control_planes
  become_user: "{{ _kube.user }}"

  tasks:
    - name: Installing python kubernetes module and dependencies for Ansible
      pip:
        name:
          - pyyaml
          - kubernetes
          - jsonpatch

    - name: Create .kube in home directory
      file:
        path: $HOME/.kube
        state: directory
        mode: 0755

    - name: Copy admin.conf to home directory kube config
      become_user: root
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ _kube.user }}/.kube/config"
        remote_src: yes
        owner: "{{ _kube.user }}"

    - name: Configure autocomplete and aliases
      block:
        - name: Add autocomplete and aliases to shell
          blockinfile:
            path: ~/.bashrc
            block: |
              source <(kubectl completion bash)
              alias kc=kubectl
              alias kcaf='kubectl apply -f'
              alias kcd='kubectl delete'
              alias kcdf='kubectl delete -f'
              alias kcpo='kubectl get po'
              alias kcpoa='kubectl get po -A'
              alias kcpon='kubectl get po -n'
              alias kcsv='kubectl get svc'
              alias kcsva='kubectl get svc -A'
              alias kcsvn='kubectl get svc -n'
              alias kcdp='kubectl get deploy'
              alias kcdpa='kubectl get deploy -A'
              alias kcdpn='kubectl get deploy -n'
              alias kcse='kubectl get secrets'
              alias kcsen='kubectl get secrets -n'
              alias kcsc='kubectl get sc'
              alias kcpv='kubectl get pv'
              alias kcpvc='kubectl get pvc'
              alias kcpvcn='kubectl get pvc -n'
              alias kcno='kubectl get no'
              alias kclo='kubectl logs'
              alias kcde='kubectl describe'
              alias kcdepo='kubectl describe po'
              alias kcdesv='kubectl describe svc'
              alias wkcpo='watch kubectl get po'
              alias wkcpoa='watch kubectl get po -A'
              alias wkcpon='watch kubectl get po -n'
              alias kcpow='kubectl get po -o wide'
              alias kcpowa='kubectl get po -o wide -A'
              alias kcpown='kubectl get po -o wide -n'
              alias kcnow='kubectl get no -o wide'
              alias kcsvw='kubectl get svc -o wide'
              alias kctpoa='kubectl top po -A'
              alias kctpon='kubectl top po -n'
              alias kctno='kubectl top no'
              complete -o default -F __start_kubectl kc

        - name: Apply changes
          shell: . ~/.bashrc

- name: Configure kubernetes nodes
  hosts: kubernetes_nodes
  vars:
    cmd_file: join_node_command

  tasks:
    - name: Copy join command from Ansible host to the nodes
      copy:
        src: "{{ _work_dir }}/{{ cmd_file }}"
        dest: "/tmp/{{ cmd_file }}"
        mode: 0777

    - name: Joining kubernetes cluster
      command: sh '/tmp/{{ cmd_file }}'

- name: Finalizing installation
  hosts: kubernetes_control_planes[0]
  become_user: "{{ _kube.user }}"

  tasks:
    - name: Installing calico
      kubernetes.core.k8s:
        definition: "{{ lookup('file', item) | from_yaml_all }}"
        wait: yes
      loop:
        - "{{ _work_dir }}/tigera-operator.yaml"
        - "{{ _work_dir }}/custom-resources.yaml"

    - name: Remove taint to allow scheduling on control plane nodes (if no nodes were defined)
      shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane-
      when: (groups['kubernetes_nodes'] | length) < 1 

    - name: Waiting for kubernetes cluster to be ready
      shell: |
             kubectl wait --for=condition=Ready nodes --all --timeout=600s
             for d in $(kubectl -n calico-system get deployment -o json | jq -r .items[].metadata.name); do kubectl rollout status -n calico-system deployment/$d; done;

    - name: Wait for calico-apiserver deployment to start
      wait_for:
        timeout: 30
      delegate_to: localhost

    - name: Waiting for calico-apiserver to be ready
      shell: |
             for d in $(kubectl -n calico-apiserver get deployment -o json | jq -r .items[].metadata.name); do kubectl rollout status -n calico-apiserver deployment/$d; done;

    - name: Cleanup manifest files
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - "{{ _work_dir }}/tigera-operator.yaml"
        - "{{ _work_dir }}/custom-resources.yaml"
      delegate_to: localhost
